{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinJayne2023/agent_recruiter_screening_langgraph/blob/main/agent_recruiter_screening_langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ade998c-79b2-4af5-9207-034e92bb9fcc"
      },
      "source": [
        "# Recruiter Screening Notes — LangChain + LangGraph\n",
        "\n",
        "Paste raw screening-call notes → get polished **Screening Notes** (Markdown) and **5 job title suggestions** with rationales.\n",
        "\n",
        "**What this notebook does**\n",
        "- Extracts structured data from messy notes (name, experience, skills, comp, flags...)\n",
        "- Normalizes skills\n",
        "- Suggests 5 job titles with brief rationales\n",
        "- Produces a final Markdown summary + JSON payload you can paste to your ATS/CRM\n",
        "\n",
        "**Stack**: LangChain, LangGraph, OpenAI (or swap model), Pydantic, RapidFuzz\n"
      ],
      "id": "8ade998c-79b2-4af5-9207-034e92bb9fcc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Install Dependencies"
      ],
      "metadata": {
        "id": "BQD3U0u7LcFV"
      },
      "id": "BQD3U0u7LcFV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1703e5e2-71fc-4568-89b4-661c85c721f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb145d5-150a-4389-934a-775e0bd117ca"
      },
      "source": [
        "!pip -q install \"langchain>=0.2.10\" \"langgraph>=0.2.20\" \"langchain-openai>=0.1.20\" \"pydantic>=2.7.0\" \"rapidfuzz>=3.7.0\" \"python-dotenv>=1.0.1\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.5/449.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "id": "1703e5e2-71fc-4568-89b4-661c85c721f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29a36060-5826-4cc3-bef3-2fcfdee214c4"
      },
      "source": [
        "## 2) Configure API key"
      ],
      "id": "29a36060-5826-4cc3-bef3-2fcfdee214c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbcd60e1-20d9-4bd4-8897-3386abc63b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7e107d-13f1-47c4-c9cb-b1c8f3927265"
      },
      "source": [
        "import os, getpass\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    try:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")\n",
        "    except Exception as e:\n",
        "        print(\"Set it manually: os.environ['OPENAI_API_KEY'] = ...\")\n",
        "print(\"API key set:\", bool(os.environ.get(\"OPENAI_API_KEY\")))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI_API_KEY··········\n",
            "API key set: True\n"
          ]
        }
      ],
      "id": "dbcd60e1-20d9-4bd4-8897-3386abc63b2c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "461f5dc2-de7c-4e8b-aed5-39b86e493152"
      },
      "source": [
        "## 3) Define models, helpers, and ontology"
      ],
      "id": "461f5dc2-de7c-4e8b-aed5-39b86e493152"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8239631-9245-4409-9a4c-eaf073a32fdf"
      },
      "source": [
        "from __future__ import annotations\n",
        "from typing import List, Optional, Dict, Any, TypedDict\n",
        "from dataclasses import dataclass\n",
        "from pydantic import BaseModel, Field\n",
        "import os, re, json\n",
        "from rapidfuzz import fuzz\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.graph import StateGraph, END\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "class ExperienceItem(BaseModel):\n",
        "    title: Optional[str] = None\n",
        "    company: Optional[str] = None\n",
        "    start: Optional[str] = None\n",
        "    end: Optional[str] = None\n",
        "    highlights: List[str] = Field(default_factory=list)\n",
        "\n",
        "class ScreeningExtraction(BaseModel):\n",
        "    candidate_name: Optional[str] = None\n",
        "    current_title: Optional[str] = None\n",
        "    current_company: Optional[str] = None\n",
        "    years_experience: Optional[float] = None\n",
        "    location: Optional[str] = None\n",
        "    work_authorization: Optional[str] = None\n",
        "    remote_on_site_pref: Optional[str] = None\n",
        "    comp_current: Optional[str] = None\n",
        "    comp_expected: Optional[str] = None\n",
        "    notice_period: Optional[str] = None\n",
        "    earliest_start_date: Optional[str] = None\n",
        "    role_preferences: List[str] = Field(default_factory=list)\n",
        "    top_skills: List[str] = Field(default_factory=list)\n",
        "    tools_stack: List[str] = Field(default_factory=list)\n",
        "    industries: List[str] = Field(default_factory=list)\n",
        "    risk_flags: List[str] = Field(default_factory=list)\n",
        "    notable_quotes: List[str] = Field(default_factory=list)\n",
        "    experience: List[ExperienceItem] = Field(default_factory=list)\n",
        "    summary_paragraph: Optional[str] = None\n",
        "\n",
        "class TitleSuggestion(BaseModel):\n",
        "    title: str\n",
        "    rationale: str\n",
        "    confidence: float = Field(ge=0, le=1)\n",
        "\n",
        "class ScreenState(TypedDict, total=False):\n",
        "    raw_notes: str\n",
        "    cleaned_notes: str\n",
        "    extracted: ScreeningExtraction\n",
        "    normalized_skills: List[str]\n",
        "    title_suggestions: List[TitleSuggestion]\n",
        "    formatted_markdown: str\n",
        "    json_payload: Dict[str, Any]\n",
        "\n",
        "def make_llm(model: str = \"gpt-4o-mini\", temperature: float = 0.1):\n",
        "    return ChatOpenAI(model=model, temperature=temperature)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "# --- Canonical skills ---\n",
        "CANONICAL_SKILLS = {\n",
        "    \"python\": [\"py\", \"python3\"],\n",
        "    \"java\": [],\n",
        "    \"javascript\": [\"js\", \"ecmascript\"],\n",
        "    \"typescript\": [\"ts\"],\n",
        "    \"react\": [\"reactjs\", \"react.js\"],\n",
        "    \"node\": [\"nodejs\", \"node.js\"],\n",
        "    \"sql\": [],\n",
        "    \"aws\": [\"amazon web services\"],\n",
        "    \"gcp\": [\"google cloud\"],\n",
        "    \"azure\": [],\n",
        "    \"docker\": [],\n",
        "    \"kubernetes\": [\"k8s\", \"kube\"],\n",
        "    \"spark\": [\"pyspark\"],\n",
        "    \"hadoop\": [],\n",
        "    \"airflow\": [],\n",
        "    \"terraform\": [],\n",
        "    \"ci/cd\": [\"cicd\", \"ci cd\"],\n",
        "    # data/ml\n",
        "    \"pandas\": [],\n",
        "    \"numpy\": [],\n",
        "    \"pytorch\": [],\n",
        "    \"tensorflow\": [\"tf\", \"tfx\"],\n",
        "    \"sklearn\": [\"scikit-learn\"],\n",
        "    \"mlops\": [\"sagemaker\", \"vertex ai\", \"mlflow\"],\n",
        "    \"nlp\": [\"natural language processing\"],\n",
        "    \"computer vision\": [\"cv\"],\n",
        "    \"llm\": [\"large language model\", \"gpt\", \"llama\"],\n",
        "    # RAG / LLM tooling\n",
        "    \"langchain\": [],\n",
        "    \"langgraph\": [],\n",
        "    \"prompt engineering\": [\"prompting\", \"system prompt\", \"few-shot\"],\n",
        "    \"rag\": [\"retrieval-augmented generation\", \"retrieval\"],\n",
        "    \"faiss\": [],\n",
        "    \"pgvector\": [],\n",
        "    \"pinecone\": [],\n",
        "    \"openai\": [],\n",
        "    \"anthropic\": [],\n",
        "    # analytics / data tools\n",
        "    \"dbt\": [],\n",
        "    # sre/devops\n",
        "    \"prometheus\": [],\n",
        "    \"grafana\": [],\n",
        "    # qa/security\n",
        "    \"selenium\": [],\n",
        "    \"cypress\": [],\n",
        "    \"pytest\": [],\n",
        "    \"security\": [],\n",
        "    \"iam\": [],\n",
        "    \"kms\": [],\n",
        "    \"siem\": [],\n",
        "}\n",
        "\n",
        "TITLE_ONTOLOGY = [\n",
        "    \"Software Engineer (Backend)\",\n",
        "    \"Software Engineer (Frontend)\",\n",
        "    \"Full-Stack Engineer\",\n",
        "    \"Data Engineer\",\n",
        "    \"Machine Learning Engineer\",\n",
        "    \"MLOps Engineer\",\n",
        "    \"Data Scientist\",\n",
        "    \"Analytics Engineer\",\n",
        "    \"DevOps Engineer\",\n",
        "    \"Site Reliability Engineer (SRE)\",\n",
        "    \"QA Automation Engineer\",\n",
        "    \"Security Engineer\",\n",
        "    \"Solutions Architect\",\n",
        "    \"Product Manager (Technical)\",\n",
        "    \"AI Engineer\",\n",
        "    \"Prompt Engineer\",\n",
        "]\n",
        "\n",
        "TITLE_KEYWORDS = {\n",
        "    \"Software Engineer (Backend)\": [\"python\", \"java\", \"node\", \"sql\", \"aws\", \"gcp\", \"docker\", \"kubernetes\"],\n",
        "    \"Software Engineer (Frontend)\": [\"javascript\", \"typescript\", \"react\"],\n",
        "    \"Full-Stack Engineer\": [\"react\", \"node\", \"python\", \"typescript\"],\n",
        "    \"Data Engineer\": [\"spark\", \"airflow\", \"python\", \"sql\", \"hadoop\", \"aws\"],\n",
        "    \"Machine Learning Engineer\": [\"pytorch\", \"tensorflow\", \"sklearn\", \"mlops\", \"llm\", \"nlp\", \"computer vision\"],\n",
        "    \"MLOps Engineer\": [\"mlops\", \"kubernetes\", \"docker\", \"aws\", \"gcp\", \"mlflow\", \"sagemaker\", \"vertex ai\"],\n",
        "    \"Data Scientist\": [\"python\", \"pandas\", \"numpy\", \"sklearn\", \"nlp\", \"llm\"],\n",
        "    \"Analytics Engineer\": [\"sql\", \"dbt\", \"warehouse\", \"etl\", \"airflow\"],\n",
        "    \"DevOps Engineer\": [\"aws\", \"gcp\", \"azure\", \"kubernetes\", \"docker\", \"terraform\", \"ci/cd\"],\n",
        "    \"Site Reliability Engineer (SRE)\": [\"kubernetes\", \"observability\", \"prometheus\", \"grafana\", \"incident\"],\n",
        "    \"QA Automation Engineer\": [\"selenium\", \"cypress\", \"pytest\"],\n",
        "    \"Security Engineer\": [\"security\", \"iam\", \"kms\", \"siem\"],\n",
        "    \"Solutions Architect\": [\"pre-sales\", \"solutions\", \"customer\", \"cloud\", \"architecture\"],\n",
        "    \"Product Manager (Technical)\": [\"product\", \"requirements\", \"backlog\", \"roadmap\", \"stakeholder\"],\n",
        "    \"AI Engineer\": [\n",
        "        \"llm\", \"rag\", \"langchain\", \"langgraph\", \"embeddings\", \"vector\", \"faiss\", \"pgvector\",\n",
        "        \"pinecone\", \"openai\", \"anthropic\", \"function calling\", \"tools\", \"agents\", \"retrieval\",\n",
        "        \"guardrails\", \"evaluation\", \"observability\", \"python\", \"pydantic\"\n",
        "    ],\n",
        "    \"Prompt Engineer\": [\n",
        "        \"prompt\", \"few-shot\", \"system prompt\", \"templates\", \"structured output\", \"json\",\n",
        "        \"function calling\", \"tools\", \"eval\", \"evaluation\", \"guardrail\", \"jailbreak\",\n",
        "        \"injection\", \"rag\", \"retrieval\", \"langchain\", \"langgraph\", \"rubrics\", \"A/B\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "def normalize_skills(skills: List[str]) -> List[str]:\n",
        "    canon = set()\n",
        "    for raw in skills:\n",
        "        if not raw:\n",
        "            continue\n",
        "        low = raw.lower().strip()\n",
        "        hit = False\n",
        "        for key, aliases in CANONICAL_SKILLS.items():\n",
        "            if low == key or low in aliases:\n",
        "                canon.add(key)\n",
        "                hit = True\n",
        "                break\n",
        "        if hit:\n",
        "            continue\n",
        "        best = None\n",
        "        best_score = 0\n",
        "        for key, aliases in CANONICAL_SKILLS.items():\n",
        "            for term in [key] + aliases:\n",
        "                score = fuzz.partial_ratio(low, term)\n",
        "                if score > best_score:\n",
        "                    best, best_score = key, score\n",
        "        if best and best_score >= 85:\n",
        "            canon.add(best)\n",
        "        else:\n",
        "            canon.add(raw)\n",
        "    return sorted(canon)\n",
        "\n",
        "def score_titles(skills: List[str], text_blob: str) -> List[tuple[str, float]]:\n",
        "    low_text = text_blob.lower()\n",
        "    scores = []\n",
        "    skillset = set([s.lower() for s in skills])\n",
        "    for title in TITLE_ONTOLOGY:\n",
        "        kws = TITLE_KEYWORDS.get(title, [])\n",
        "        hit_skill = len(skillset.intersection(set(kws)))\n",
        "        hit_text = sum(1 for k in kws if k in low_text)\n",
        "        score = hit_skill * 1.0 + hit_text * 0.5\n",
        "        scores.append((title, score))\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scores\n"
      ],
      "execution_count": 18,
      "outputs": [],
      "id": "d8239631-9245-4409-9a4c-eaf073a32fdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64b3bbb3-c118-4457-8cd5-516347ff1a03"
      },
      "source": [
        "## 4) Build the LangGraph and nodes"
      ],
      "id": "64b3bbb3-c118-4457-8cd5-516347ff1a03"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9449b7c1-4fa7-474e-88c7-680d05052dc1"
      },
      "source": [
        "def preprocess_node(state: ScreenState) -> ScreenState:\n",
        "    notes = state.get(\"raw_notes\", \"\")\n",
        "    cleaned = normalize_text(notes)\n",
        "    return {\"cleaned_notes\": cleaned}\n",
        "\n",
        "def extract_node(state: ScreenState) -> ScreenState:\n",
        "    llm = make_llm()\n",
        "    schema_json = ScreeningExtraction.schema_json(indent=2)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "         \"You are a meticulous recruiter’s assistant. Extract structured fields from the call notes. \"\n",
        "         \"If a field is unknown, leave it null or empty. Do not invent facts.\"),\n",
        "        (\"human\",\n",
        "         \"Call notes:\\n\\n{notes}\\n\\n\"\n",
        "         \"Return ONLY a JSON object matching this Pydantic schema:\\n{schema}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm.with_structured_output(ScreeningExtraction)\n",
        "    extracted: ScreeningExtraction = chain.invoke({\n",
        "        \"notes\": state[\"cleaned_notes\"],\n",
        "        \"schema\": schema_json\n",
        "    })\n",
        "    return {\"extracted\": extracted}\n",
        "\n",
        "\n",
        "def normalize_skills_node(state: ScreenState) -> ScreenState:\n",
        "    extracted: ScreeningExtraction = state[\"extracted\"]\n",
        "    combined = list({*(extracted.top_skills or []), *(extracted.tools_stack or [])})\n",
        "    norm = normalize_skills(combined)\n",
        "    return {\"normalized_skills\": norm}\n",
        "\n",
        "def suggest_titles_node(state: ScreenState) -> ScreenState:\n",
        "    extracted: ScreeningExtraction = state[\"extracted\"]\n",
        "    skills = state.get(\"normalized_skills\", [])\n",
        "\n",
        "    text_blob = \" \".join([\n",
        "        state.get(\"cleaned_notes\", \"\"),\n",
        "        \" \".join(skills),\n",
        "        extracted.summary_paragraph or \"\",\n",
        "        (extracted.current_title or \"\") + \" \" + \" \".join(extracted.role_preferences or [])\n",
        "    ])\n",
        "    base_scores = score_titles(skills, text_blob)\n",
        "    top8 = [t for t, s in base_scores[:8] if s > 0] or [t for t, _ in base_scores[:8]]\n",
        "\n",
        "    llm = make_llm(temperature=0.2)\n",
        "    context_json = extracted.json()\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You propose 5 job titles a candidate fits and explain why. Calibrate confidence 0.6–0.95.\"),\n",
        "        (\"human\",\n",
        "         \"Candidate context (JSON):\\n{context_json}\\n\\n\"\n",
        "         \"Normalized skills: {skills}\\n\"\n",
        "         \"Title candidates to choose from: {top8}\\n\\n\"\n",
        "         \"Return a JSON array of 5 objects with keys: title, rationale, confidence.\")\n",
        "    ])\n",
        "\n",
        "    parser = StrOutputParser()\n",
        "    raw = (prompt | llm | parser).invoke({\n",
        "        \"context_json\": context_json,\n",
        "        \"skills\": skills,\n",
        "        \"top8\": top8\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(raw)\n",
        "        suggestions = [TitleSuggestion(**d) for d in parsed][:5]\n",
        "    except Exception:\n",
        "        suggestions = [\n",
        "            TitleSuggestion(title=t, rationale=\"Matches skill keywords and experience signals.\", confidence=0.7)\n",
        "            for t, _ in base_scores[:5]\n",
        "        ]\n",
        "    return {\"title_suggestions\": suggestions}\n",
        "\n",
        "def format_markdown_node(state: ScreenState) -> ScreenState:\n",
        "    e: ScreeningExtraction = state[\"extracted\"]\n",
        "    titles: List[TitleSuggestion] = state[\"title_suggestions\"]\n",
        "    skills = \", \".join(state.get(\"normalized_skills\", []))\n",
        "\n",
        "    md = []\n",
        "    md.append(f\"# Screening Notes — {e.candidate_name or 'Candidate'}\")\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Overview\")\n",
        "    md.append(e.summary_paragraph or \"—\")\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Snapshot\")\n",
        "    md.append(f\"- **Current Title/Company:** {e.current_title or '—'} @ {e.current_company or '—'}\")\n",
        "    md.append(f\"- **Location:** {e.location or '—'}\")\n",
        "    md.append(f\"- **Years Experience:** {e.years_experience if e.years_experience is not None else '—'}\")\n",
        "    md.append(f\"- **Work Authorization:** {e.work_authorization or '—'}\")\n",
        "    md.append(f\"- **Work Preference:** {e.remote_on_site_pref or '—'}\")\n",
        "    md.append(f\"- **Comp (Current):** {e.comp_current or '—'}\")\n",
        "    md.append(f\"- **Comp (Expected):** {e.comp_expected or '—'}\")\n",
        "    md.append(f\"- **Notice / Availability:** {e.notice_period or '—'} / {e.earliest_start_date or '—'}\")\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Role Preferences\")\n",
        "    md.append((\"; \".join(e.role_preferences)) if e.role_preferences else \"—\")\n",
        "    md.append(\"\")\n",
        "    md.append(\"## Skills & Stack\")\n",
        "    md.append(skills or \"—\")\n",
        "    md.append(\"\")\n",
        "    if e.experience:\n",
        "        md.append(\"## Experience Highlights\")\n",
        "        for item in e.experience:\n",
        "            line = f\"- **{item.title or '—'}**, {item.company or '—'}\"\n",
        "            if item.start or item.end:\n",
        "                line += f\" ({item.start or ''}–{item.end or ''})\"\n",
        "            md.append(line)\n",
        "            for h in (item.highlights or []):\n",
        "                md.append(f\"  - {h}\")\n",
        "        md.append(\"\")\n",
        "    if e.risk_flags:\n",
        "        md.append(\"## Risks / Concerns\")\n",
        "        for r in e.risk_flags:\n",
        "            md.append(f\"- {r}\")\n",
        "        md.append(\"\")\n",
        "    if e.notable_quotes:\n",
        "        md.append(\"## Notable Quotes\")\n",
        "        for q in e.notable_quotes:\n",
        "            md.append(f\"> {q}\")\n",
        "        md.append(\"\")\n",
        "    md.append(\"## Suggested Job Titles (Top 5)\")\n",
        "    for i, t in enumerate(titles[:5], 1):\n",
        "        md.append(f\"{i}. **{t.title}** — {t.rationale} _(confidence: {t.confidence:.2f})_\")\n",
        "\n",
        "    payload = {\n",
        "        \"extracted\": json.loads(e.model_dump_json()),\n",
        "        \"normalized_skills\": state.get(\"normalized_skills\", []),\n",
        "        \"title_suggestions\": [t.model_dump() for t in titles[:5]],\n",
        "    }\n",
        "    return {\"formatted_markdown\": \"\\n\".join(md), \"json_payload\": payload}\n",
        "\n",
        "def build_graph():\n",
        "    sg = StateGraph(ScreenState)\n",
        "    sg.add_node(\"preprocess\", preprocess_node)\n",
        "    sg.add_node(\"extract\", extract_node)\n",
        "    sg.add_node(\"normalize_skills\", normalize_skills_node)\n",
        "    sg.add_node(\"suggest_titles\", suggest_titles_node)\n",
        "    sg.add_node(\"format_markdown\", format_markdown_node)\n",
        "    sg.set_entry_point(\"preprocess\")\n",
        "    sg.add_edge(\"preprocess\", \"extract\")\n",
        "    sg.add_edge(\"extract\", \"normalize_skills\")\n",
        "    sg.add_edge(\"normalize_skills\", \"suggest_titles\")\n",
        "    sg.add_edge(\"suggest_titles\", \"format_markdown\")\n",
        "    sg.add_edge(\"format_markdown\", END)\n",
        "    return sg.compile()"
      ],
      "execution_count": 19,
      "outputs": [],
      "id": "9449b7c1-4fa7-474e-88c7-680d05052dc1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01fab2f9-ce72-46fa-b2ae-6b4fba2cabc7"
      },
      "source": [
        "## 5) Paste your call notes and run\n",
        "Enter redacted notes below"
      ],
      "id": "01fab2f9-ce72-46fa-b2ae-6b4fba2cabc7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e699bf2-3ff3-4666-ac85-97b46c750a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e74835-e299-4b19-a40c-e854f4a37745"
      },
      "source": [
        "RAW_NOTES = \"\"\"\n",
        "Add notes here\n",
        "\"\"\".strip()\n",
        "print(RAW_NOTES[:1000] + (\"...\" if len(RAW_NOTES)>1000 else \"\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add notes here\n"
          ]
        }
      ],
      "id": "2e699bf2-3ff3-4666-ac85-97b46c750a24"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce0de65-de8c-420a-a14d-faf4e0c88beb"
      },
      "source": [
        "## 6) Run the graph -> View results (Markdown + JSON)"
      ],
      "id": "7ce0de65-de8c-420a-a14d-faf4e0c88beb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f03f3fa-3332-4cf7-8a38-a4d69d3b6222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "collapsed": true,
        "outputId": "04a926ad-4064-4442-d04f-2c0d832cc776"
      },
      "source": [
        "graph = build_graph()\n",
        "out = graph.invoke({\"raw_notes\": RAW_NOTES})\n",
        "md = out[\"formatted_markdown\"]\n",
        "payload = out[\"json_payload\"]\n",
        "display(Markdown(md))\n",
        "print(\"\\n--- JSON payload ---\\n\")\n",
        "print(json.dumps(payload, indent=2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-770463688.py:8: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  schema_json = ScreeningExtraction.schema_json(indent=2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3269648552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"raw_notes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRAW_NOTES\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"formatted_markdown\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"json_payload\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2655\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-770463688.py\u001b[0m in \u001b[0;36mextract_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_structured_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScreeningExtraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     extracted: ScreeningExtraction = chain.invoke({\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;34m\"notes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cleaned_notes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m\"schema\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mschema_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3245\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3246\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3247\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5709\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5710\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5711\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5712\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5713\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m         return cast(\n\u001b[1;32m    394\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1024\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 results.append(\n\u001b[0;32m--> 842\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    843\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         if (\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     raw_response = (\n\u001b[0;32m-> 1145\u001b[0;31m                         self.root_client.chat.completions.with_raw_response.parse(\n\u001b[0m\u001b[1;32m   1146\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "id": "5f03f3fa-3332-4cf7-8a38-a4d69d3b6222"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304aabbe-a9a4-4024-bf7b-6201e81efa5c"
      },
      "source": [
        "## 7) Save to files\n",
        "Saves Markdown and JSON to `/content/`. You can also upload them to your ATS."
      ],
      "id": "304aabbe-a9a4-4024-bf7b-6201e81efa5c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5c5acc7-5cd7-4319-9826-0a4dcdc5b803"
      },
      "source": [
        "from pathlib import Path\n",
        "Path('/content').mkdir(parents=True, exist_ok=True)\n",
        "md_path = '/content/screening_notes.md'\n",
        "json_path = '/content/screening_payload.json'\n",
        "with open(md_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(md)\n",
        "with open(json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "print('Saved:', md_path)\n",
        "print('Saved:', json_path)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a5c5acc7-5cd7-4319-9826-0a4dcdc5b803"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Gradio"
      ],
      "metadata": {
        "id": "1I89yoX1buZu"
      },
      "id": "1I89yoX1buZu"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio>=4.44.0\n"
      ],
      "metadata": {
        "id": "RN-0ymmPMnrO"
      },
      "id": "RN-0ymmPMnrO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio UI for Screening Notes"
      ],
      "metadata": {
        "id": "DNA-YCXIhEtn"
      },
      "id": "DNA-YCXIhEtn"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gradio UI for Screening Notes (optional file saving) ---\n",
        "import os, json, tempfile\n",
        "import gradio as gr\n",
        "\n",
        "DEFAULT_SAMPLE = (\n",
        "    \"Candidate: Alex Kim. Based in Austin, open to hybrid. US Citizen.\\n\"\n",
        "    \"6.5 years experience. Current: Senior Data Engineer @ FinTechCo (Python, Spark, Airflow, AWS, Terraform).\\n\"\n",
        "    \"Built streaming ETL on Kinesis; led migration to Snowflake; strong SQL. Interested in ML platform + LLM/RAG work.\\n\"\n",
        "    \"Current comp: 165k base + 10% bonus. Target 180–200k total. Notice: 2 weeks.\\n\"\n",
        "    \"Prefers IC roles, hands-on; not interested in pure analytics. Led a squad of 3.\\n\"\n",
        "    \"Tools: GitHub Actions, Docker, dbt, LangChain, LangGraph. Risk: limited real-time MLOps, more batch.\\n\"\n",
        "    \"Quote: 'I love making pipelines boring and reliable.'\"\n",
        ")\n",
        "\n",
        "def run_screening(notes: str, enable_downloads: bool):\n",
        "    if not notes or not notes.strip():\n",
        "        return (\"Please paste screening notes.\", {}, gr.update(visible=False), gr.update(visible=False))\n",
        "\n",
        "    out = graph.invoke({\"raw_notes\": notes})\n",
        "    md = out[\"formatted_markdown\"]\n",
        "    payload = out[\"json_payload\"]\n",
        "\n",
        "    if enable_downloads:\n",
        "        # temp files user can download\n",
        "        md_fd, md_path = tempfile.mkstemp(suffix=\".md\")\n",
        "        with os.fdopen(md_fd, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(md)\n",
        "        json_fd, json_path = tempfile.mkstemp(suffix=\".json\")\n",
        "        with os.fdopen(json_fd, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(payload, f, indent=2)\n",
        "        return (md, payload, gr.update(value=md_path, visible=True), gr.update(value=json_path, visible=True))\n",
        "    else:\n",
        "        return (md, payload, gr.update(value=None, visible=False), gr.update(value=None, visible=False))\n",
        "\n",
        "with gr.Blocks(title=\"Screening Notes (LangGraph)\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Screening Notes — LangChain + LangGraph\n",
        "        Paste your call notes below. Click **Generate** to produce formatted notes and title suggestions.\n",
        "        Use the toggle if you want downloadable files (optional).\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        notes = gr.Textbox(\n",
        "            label=\"Paste screening notes\",\n",
        "            value=DEFAULT_SAMPLE,\n",
        "            lines=16,\n",
        "            show_copy_button=True,\n",
        "            placeholder=\"Paste your raw call notes here...\"\n",
        "        )\n",
        "    with gr.Row():\n",
        "        enable_downloads = gr.Checkbox(label=\"Enable downloads (save temp files)\", value=False)\n",
        "    btn = gr.Button(\"Generate\", variant=\"primary\")\n",
        "\n",
        "    md_out = gr.Markdown(label=\"Formatted Screening Notes\")\n",
        "    json_out = gr.JSON(label=\"JSON Payload\")\n",
        "\n",
        "    with gr.Row():\n",
        "        md_file = gr.File(label=\"Download Markdown\", visible=False)\n",
        "        json_file = gr.File(label=\"Download JSON\", visible=False)\n",
        "\n",
        "    btn.click(\n",
        "        fn=run_screening,\n",
        "        inputs=[notes, enable_downloads],\n",
        "        outputs=[md_out, json_out, md_file, json_file]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "V98hGiuOb0yg"
      },
      "id": "V98hGiuOb0yg",
      "execution_count": null,
      "outputs": []
    }
  ]
}